title("Cross-Validation for Ridge Regression", line = 2.5)
target <- ncol(train_data)
index <- createDataPartition(train_data[[target]], p = 0.8, list = FALSE)
train_set <- train_data[index, ]
validation_set <- train_data[-index, ]
# 4. Dimensionality Reduction
# Principal Component Regression (PCR)
pcr_model <- pcr(train_set[[target]] ~ ., data = train_set, validation = "CV")
pcr_pred <- predict(pcr_model, validation_set, ncomp = which.min(pcr_model$validation$PRESS))
pcr_rmse <- sqrt(mean((validation_set[[target]] - pcr_pred)^2))
# Partial Least Squares (PLS)
pls_model <- plsr(train_set[[target]] ~ ., data = train_set, validation = "CV")
# Load necessary libraries
library(glmnet)
library(caret)
library(pls)
library(splines)
# 2. Load and Prepare Data
train_data <- read.csv("blogData_train.csv", header = FALSE)
test_data <- read.csv("blogData_test-2012.03.01.00_00.csv", header = FALSE)
set.seed(129190)
# Take a random sample of 10,000 rows from train_data
train_data <- train_data[sample(nrow(train_data), 500), ]
# View the first few rows of the sampled dataset
head(train_data)
# Check the dimensions of the sampled dataset
dim(train_data)
response_variable <- train_data[, 281]  # Target variable
X <- train_data[, -281]  # Predictors
train_x <- as.matrix(train_data[,-281])  # Predictor variables in training set
train_y <- train_data[, 281]             # Response variable in training set
test_x <- as.matrix(test_data[, -281])   # Predictor variables in testing set
test_y <- test_data[, 281]               # Response variable in testing set
# Perform 5-fold cross-validation on the training set to select the best λ
cv_model <- cv.glmnet(train_x, train_y, alpha = 1, nfolds = 5)  # alpha = 1 for Lasso
# Extract the best λ
best_lambda <- cv_model$lambda.min
# Fit the final Lasso model using the best λ
lasso_model <- glmnet(train_x, train_y, alpha = 1, lambda = best_lambda)
# Make predictions on the test data
lasso_predictions <- predict(lasso_model, newx = test_x)
# Evaluate performance on the test data
lasso_mse <- mean((lasso_predictions - test_y)^2)  # Mean Squared Error
cat("Best λ (lambda.min):", best_lambda, "\n")
cat("Mean Squared Error on Test Data:", lasso_mse, "\n")
# Perform cross-validation for Lasso regression (alpha = 1)
cv_lasso_model <- cv.glmnet(train_x, train_y, alpha = 1, nfolds = 5)
# Cross-validation plot for Lasso
plot(cv_lasso_model)
title("Cross-Validation for Lasso Regression", line = 2.5)
####################################################################################
# Load required library
library(glmnet)
# Perform 5-fold cross-validation on the training set to select the best λ
cv_model <- cv.glmnet(train_x, train_y, alpha = 0, nfolds = 5)  # alpha = 0 for Ridge
# Extract the best λ
best_lambda <- cv_model$lambda.min
# Fit the final Ridge model using the best λ
ridge_model <- glmnet(train_x, train_y, alpha = 0, lambda = best_lambda)
# Make predictions on the test data
ridge_predictions <- predict(ridge_model, newx = test_x)
# Evaluate performance on the test data
ridge_mse <- mean((ridge_predictions - test_y)^2)  # Mean Squared Error
cat("Best λ (lambda.min):", best_lambda, "\n")
cat("Mean Squared Error on Test Data:", ridge_mse, "\n")
# Cross-validation plot
plot(cv_model)
title("Cross-Validation for Ridge Regression", line = 2.5)
target <- ncol(train_data)
index <- createDataPartition(train_data[[target]], p = 0.8, list = FALSE)
train_set <- train_data[index, ]
validation_set <- train_data[-index, ]
# 4. Dimensionality Reduction
# Principal Component Regression (PCR)
pcr_model <- pcr(train_set[[target]] ~ ., data = train_set, validation = "CV")
pcr_pred <- predict(pcr_model, validation_set, ncomp = which.min(pcr_model$validation$PRESS))
pcr_rmse <- sqrt(mean((validation_set[[target]] - pcr_pred)^2))
# Partial Least Squares (PLS)
pls_model <- plsr(train_set[[target]] ~ ., data = train_set, validation = "CV")
pls_pred <- predict(pls_model, validation_set, ncomp = which.min(pls_model$validation$PRESS))
pls_rmse <- sqrt(mean((validation_set[[target]] - pls_pred)^2))
# 5. Model Comparison
model_comparison <- data.frame(
Model = c("Lasso", "Ridge", "PCR", "PLS"),
RMSE = c(lasso_rmse, ridge_rmse, pcr_rmse, pls_rmse)
)
train_x <- as.matrix(train_data[,-281])  # Predictor variables in training set
train_y <- train_data[, 281]             # Response variable in training set
test_x <- as.matrix(test_data[, -281])   # Predictor variables in testing set
test_y <- test_data[, 281]               # Response variable in testing set
# Perform 5-fold cross-validation on the training set to select the best λ
cv_model <- cv.glmnet(train_x, train_y, alpha = 1, nfolds = 5)  # alpha = 1 for Lasso
# Extract the best λ
best_lambda <- cv_model$lambda.min
# Fit the final Lasso model using the best λ
lasso_model <- glmnet(train_x, train_y, alpha = 1, lambda = best_lambda)
# Make predictions on the test data
lasso_predictions <- predict(lasso_model, newx = test_x)
# Evaluate performance on the test data
lasso_mse <- mean((lasso_predictions - test_y)^2)  # Mean Squared Error
cat("Best λ (lambda.min):", best_lambda, "\n")
cat("Mean Squared Error on Test Data:", lasso_mse, "\n")
# Perform cross-validation for Lasso regression (alpha = 1)
cv_lasso_model <- cv.glmnet(train_x, train_y, alpha = 1, nfolds = 5)
# Cross-validation plot for Lasso
plot(cv_lasso_model)
title("Cross-Validation for Lasso Regression", line = 2.5)
####################################################################################
# Load required library
library(glmnet)
# Perform 5-fold cross-validation on the training set to select the best λ
cv_model <- cv.glmnet(train_x, train_y, alpha = 0, nfolds = 5)  # alpha = 0 for Ridge
# Extract the best λ
best_lambda <- cv_model$lambda.min
# Fit the final Ridge model using the best λ
ridge_model <- glmnet(train_x, train_y, alpha = 0, lambda = best_lambda)
# Make predictions on the test data
ridge_predictions <- predict(ridge_model, newx = test_x)
# Evaluate performance on the test data
ridge_mse <- mean((ridge_predictions - test_y)^2)  # Mean Squared Error
cat("Best λ (lambda.min):", best_lambda, "\n")
cat("Mean Squared Error on Test Data:", ridge_mse, "\n")
# Cross-validation plot
plot(cv_model)
title("Cross-Validation for Ridge Regression", line = 2.5)
target <- ncol(train_data)
index <- createDataPartition(train_data[[target]], p = 0.8, list = FALSE)
train_set <- train_data[index, ]
validation_set <- train_data[-index, ]
lasso_rmse <- sqrt(mean((validation_set[[target]] - pred_lasso)^2))
# Load necessary libraries
library(glmnet)
library(caret)
library(pls)
library(splines)
# 1. Random Seed Setup
student_id <- 123456  # Replace with your student ID
set.seed(student_id)
# 2. Load and Prepare Data
train_data <- read.csv("C:/Users/HP/Desktop/intro/blogData_train.csv", header = FALSE)
test_data <- read.csv("C:/Users/HP/Desktop/intro/test.csv", header = FALSE)
# Take a random sample of 10,000 rows from train_data
train_data <- train_data[sample(nrow(train_data), 500), ]
# View the first few rows of the sampled dataset
head(train_data)
# Check the dimensions of the sampled dataset
dim(train_data)
# Handle Missing Data
train_data[is.na(train_data)] <- 0
test_data[is.na(test_data)] <- 0
# Split Data (if needed)
# Assuming the target variable is the last column
target <- ncol(train_data)
index <- createDataPartition(train_data[[target]], p = 0.8, list = FALSE)
train_set <- train_data[index, ]
validation_set <- train_data[-index, ]
# 3. Penalized Regression Models
# Lasso Regression
lasso_cv <- cv.glmnet(as.matrix(train_set[,-target]), train_set[[target]], alpha = 1)
best_lambda_lasso <- lasso_cv$lambda.min
lasso_model <- glmnet(as.matrix(train_set[,-target]), train_set[[target]], alpha = 1, lambda = best_lambda_lasso)
pred_lasso <- predict(lasso_model, as.matrix(validation_set[,-target]))
lasso_rmse <- sqrt(mean((validation_set[[target]] - pred_lasso)^2))
# Cross-validation plot for Lasso
plot(lasso_cv)
title("Cross-Validation for Lasso Regression", line = 2.5)
# Load necessary libraries
library(glmnet)
# Ridge Regression
ridge_cv <- cv.glmnet(as.matrix(train_set[,-target]), train_set[[target]], alpha = 0)
best_lambda_ridge <- ridge_cv$lambda.min
ridge_model <- glmnet(as.matrix(train_set[,-target]), train_set[[target]], alpha = 0, lambda = best_lambda_ridge)
pred_ridge <- predict(ridge_model, as.matrix(validation_set[,-target]))
# Compute RMSE for Ridge
ridge_rmse <- sqrt(mean((validation_set[[target]] - pred_ridge)^2))
cat("Ridge RMSE:", ridge_rmse, "\n")
# 1. Cross-validation plot for Ridge
plot(ridge_cv)
title("Cross-Validation for Ridge Regression", line = 2.5)
# 2. Coefficient path plot for Ridge
plot(ridge_model, xvar = "lambda", label = TRUE)
title("Coefficient Path for Ridge Regression", line = 2.5)
# 3. Predicted vs Actual plot for Ridge
plot(validation_set[[target]], pred_ridge, main = "Predicted vs Actual (Ridge Regression)",
xlab = "Actual", ylab = "Predicted", col = "blue", pch = 16)
abline(a = 0, b = 1, col = "red")  # Add a 45-degree reference line
# 4. Dimensionality Reduction
# Principal Component Regression (PCR)
pcr_model <- pcr(train_set[[target]] ~ ., data = train_set, validation = "CV")
pcr_pred <- predict(pcr_model, validation_set, ncomp = which.min(pcr_model$validation$PRESS))
pcr_rmse <- sqrt(mean((validation_set[[target]] - pcr_pred)^2))
# Partial Least Squares (PLS)
pls_model <- plsr(train_set[[target]] ~ ., data = train_set, validation = "CV")
pls_pred <- predict(pls_model, validation_set, ncomp = which.min(pls_model$validation$PRESS))
pls_rmse <- sqrt(mean((validation_set[[target]] - pls_pred)^2))
# 5. Model Comparison
model_comparison <- data.frame(
Model = c("Lasso", "Ridge", "PCR", "PLS"),
RMSE = c(lasso_rmse, ridge_rmse, pcr_rmse, pls_rmse)
)
print(model_comparison)
# 6. Additional Models
# k-Nearest Neighbors
knn_model <- train(train_set[,-target], train_set[[target]], method = "knn", tuneLength = 10)
knn_pred <- predict(knn_model, validation_set[,-target])
knn_rmse <- sqrt(mean((validation_set[[target]] - knn_pred)^2))
#7.# 7. Polynomial Regression (Degree 2)
poly_model <- lm(train_set[[target]] ~ poly(train_set[, 1], degree = 2))  # Polynomial regression on first predictor
poly_pred <- predict(poly_model, validation_set)
poly_rmse <- sqrt(mean((validation_set[[target]] - poly_pred)^2))  # RMSE for Polynomial Regression
#8.# 8. Step Function Regression
# Creating step function based on the first predictor variable (for example)
step_model <- lm(train_set[[target]] ~ cut(train_set[, 1], breaks = 5))  # Create 5 intervals for step function
step_pred <- predict(step_model, validation_set)
step_rmse <- sqrt(mean((validation_set[[target]] - step_pred)^2))  # RMSE for Step Function Regression
#9.# 9. Cubic Splines Regression
# Fit cubic spline model on the first predictor variable
cubic_spline_model <- lm(train_set[[target]] ~ ns(train_set[, 1], df = 4))  # Using 4 degrees of freedom for the spline
cubic_spline_pred <- predict(cubic_spline_model, validation_set)
cubic_spline_rmse <- sqrt(mean((validation_set[[target]] - cubic_spline_pred)^2))  # RMSE for Cubic Splines
#10 Natural Cubic Splines
natural_spline_model <- lm(train_set[[target]] ~ ns(train_set[, 1], df = 5, Boundary.knots = c(min(train_set[, 1]), max(train_set[, 1]))))
# Predict on validation data
natural_spline_pred <- predict(natural_spline_model, newdata = validation_set)
# Calculate RMSE
natural_spline_rmse <- sqrt(mean((validation_set[[target]] - natural_spline_pred)^2))
# 11. Combine RMSE values of all models
model_comparison_all <- data.frame(
Model = c("Lasso", "Ridge", "PCR", "PLS", "KNN", "Polynomial Regression",
"Step Function Regression", "Cubic Splines", "Natural Cubic Splines"),
RMSE = c(lasso_rmse, ridge_rmse, pcr_rmse, pls_rmse, knn_rmse, poly_rmse,
step_rmse, cubic_spline_rmse, natural_spline_rmse)
)
# Display the performance comparison table
print(model_comparison_all)
# Optionally, sort by RMSE to identify the best-performing model
model_comparison_all <- model_comparison_all[order(model_comparison_all$RMSE), ]
# Print sorted model comparison
print(model_comparison_all)
# Load necessary libraries
library(glmnet)
library(caret)
library(pls)
library(splines)
# 1. Random Seed Setup
student_id <- 137138  # Replace with your student ID
set.seed(student_id)
# 2. Load and Prepare Data
train_data <- read.csv("blogData_train.csv")
test_data <- read.csv("blogData_test-2012.03.01.00_00.csv")
# Load necessary libraries
library(glmnet)
library(caret)
library(pls)
library(splines)
# 1. Random Seed Setup
student_id <- 137138  # Replace with your student ID
set.seed(student_id)
# 2. Load and Prepare Data
train_data <- read.csv("C:/Users/HP/Desktop/intro/blogData_train.csv", header = FALSE)
test_data <- read.csv("C:/Users/HP/Desktop/intro/test.csv", header = FALSE)
# Limit data to the first 10,000 rows
train_data <- train_data[1:500, ]
test_data <- test_data[1:500, ]
# Handle Missing Data
train_data[is.na(train_data)] <- 0
test_data[is.na(test_data)] <- 0
# Target is column 281, all other columns are predictors
target <- 281
predictors <- setdiff(1:ncol(train_data), target)  # All columns except target column
# 3. Lasso Regression
lasso_cv <- cv.glmnet(as.matrix(train_data[, predictors]), train_data[[target]], alpha = 1)
best_lambda_lasso <- lasso_cv$lambda.min
lasso_model <- glmnet(as.matrix(train_data[, predictors]), train_data[[target]], alpha = 1, lambda = best_lambda_lasso)
pred_lasso <- predict(lasso_model, as.matrix(test_data[, predictors]))  # Predictions on test_data
lasso_rmse <- sqrt(mean((test_data[[target]] - pred_lasso)^2))  # RMSE on test_data
# 4. Ridge Regression
ridge_cv <- cv.glmnet(as.matrix(train_data[, predictors]), train_data[[target]], alpha = 0)
best_lambda_ridge <- ridge_cv$lambda.min
ridge_model <- glmnet(as.matrix(train_data[, predictors]), train_data[[target]], alpha = 0, lambda = best_lambda_ridge)
pred_ridge <- predict(ridge_model, as.matrix(test_data[, predictors]))  # Predictions on test_data
ridge_rmse <- sqrt(mean((test_data[[target]] - pred_ridge)^2))  # RMSE on test_data
# Print the results for Lasso and Ridge
cat("Lasso RMSE:", lasso_rmse, "\n")
cat("Ridge RMSE:", ridge_rmse, "\n")
# 5. Dimensionality Reduction
# Principal Component Regression (PCR)
pcr_model <- pcr(train_data[[target]] ~ ., data = train_data, validation = "CV")
pcr_pred <- predict(pcr_model, test_data, ncomp = which.min(pcr_model$validation$PRESS))  # Predictions on test_data
pcr_rmse <- sqrt(mean((test_data[[target]] - pcr_pred)^2))  # RMSE on test_data
# Partial Least Squares (PLS)
pls_model <- plsr(train_data[[target]] ~ ., data = train_data, validation = "CV")
pls_pred <- predict(pls_model, test_data, ncomp = which.min(pls_model$validation$PRESS))  # Predictions on test_data
pls_rmse <- sqrt(mean((test_data[[target]] - pls_pred)^2))  # RMSE on test_data
# Print the results
cat("PCR RMSE:", pcr_rmse, "\n")
cat("PLS RMSE:", pls_rmse, "\n")
# 6. Model Comparison
model_comparison <- data.frame(
Model = c("Lasso", "Ridge", "PCR", "PLS"),
RMSE = c(lasso_rmse, ridge_rmse, pcr_rmse, pls_rmse)
)
print(model_comparison)
# 7. Additional Models
# k-Nearest Neighbors
knn_model <- train(train_data[, predictors], train_data[[target]], method = "knn", tuneLength = 10)
knn_pred <- predict(knn_model, test_data[, predictors])
knn_rmse <- sqrt(mean((test_data[[target]] - knn_pred)^2))
cat("KNN RMSE:", knn_rmse, "\n")
# Polynomial Regression
poly_model <- lm(train_data[[target]] ~ poly(train_data[, predictors[1]], degree = 2), data = train_data)
# Step Function Regression
step_model <- lm(train_data[[target]] ~ cut(train_data[, predictors[1]], breaks = 5), data = train_data)
# Cubic Splines
cubic_spline_model <- lm(train_data[[target]] ~ ns(train_data[, predictors[1]], df = 5), data = train_data)
# Natural Cubic Splines
natural_spline_model <- lm(train_data[[target]] ~ ns(train_data[, predictors[1]], df = 5, Boundary.knots = c(min(train_data[, predictors[1]]), max(train_data[, predictors[1]]))))
#9.# 9. Cubic Splines Regression
# Fit cubic spline model on the first predictor variable
cubic_spline_model <- lm(train_set[[target]] ~ ns(train_set[, 1], df = 4))  # Using 4 degrees of freedom for the spline
cubic_spline_pred <- predict(cubic_spline_model, validation_set)
cubic_spline_rmse <- sqrt(mean((validation_set[[target]] - cubic_spline_pred)^2))  # RMSE for Cubic Splines
# Load necessary libraries
library(glmnet)
library(caret)
library(pls)
library(splines)
set.seed(129190)
# 2. Load and Prepare Data
train_data <- read.csv("C:/Users/HP/Desktop/intro/blogData_train.csv", header = FALSE)
test_data <- read.csv("C:/Users/HP/Desktop/intro/test.csv", header = FALSE)
# Take a random sample of 10,00 rows from train_data
train_data <- train_data[sample(nrow(train_data), 1000), ]
# View the first few rows of the sampled dataset
head(train_data)
# Check the dimensions of the sampled dataset
dim(train_data)
# Handle Missing Data
train_data[is.na(train_data)] <- 0
test_data[is.na(test_data)] <- 0
target <- ncol(train_data)
index <- createDataPartition(train_data[[target]], p = 0.8, list = FALSE)
train_set <- train_data[index, ]
validation_set <- train_data[-index, ]
# 3. Penalized Regression Models
# Lasso Regression
lasso_cv <- cv.glmnet(as.matrix(train_set[,-target]), train_set[[target]], alpha = 1)
best_lambda_lasso <- lasso_cv$lambda.min
lasso_model <- glmnet(as.matrix(train_set[,-target]), train_set[[target]], alpha = 1, lambda = best_lambda_lasso)
pred_lasso <- predict(lasso_model, as.matrix(validation_set[,-target]))
lasso_rmse <- sqrt(mean((validation_set[[target]] - pred_lasso)^2))
lasso_rmse
# Cross-validation plot for Lasso
plot(lasso_cv)
title("Cross-Validation for Lasso Regression", line = 2.5)
# Load necessary libraries
library(glmnet)
# Ridge Regression
ridge_cv <- cv.glmnet(as.matrix(train_set[,-target]), train_set[[target]], alpha = 0)
best_lambda_ridge <- ridge_cv$lambda.min
ridge_model <- glmnet(as.matrix(train_set[,-target]), train_set[[target]], alpha = 0, lambda = best_lambda_ridge)
pred_ridge <- predict(ridge_model, as.matrix(validation_set[,-target]))
# Compute RMSE for Ridge
ridge_rmse <- sqrt(mean((validation_set[[target]] - pred_ridge)^2))
cat("Ridge RMSE:", ridge_rmse, "\n")
#plot
plot(ridge_cv)
title("Cross-Validation for Ridge Regression", line = 2.5)
plot(ridge_model, xvar = "lambda", label = TRUE)
title("Coefficient Path for Ridge Regression", line = 2.5)
plot(validation_set[[target]], pred_ridge, main = "Predicted vs Actual (Ridge Regression)",
xlab = "Actual", ylab = "Predicted", col = "blue", pch = 16)
abline(a = 0, b = 1, col = "red")  # Add a 45-degree reference line
# Load necessary libraries
library(glmnet)
# Ridge Regression
ridge_cv <- cv.glmnet(as.matrix(train_set[,-target]), train_set[[target]], alpha = 0)
best_lambda_ridge <- ridge_cv$lambda.min
ridge_model <- glmnet(as.matrix(train_set[,-target]), train_set[[target]], alpha = 0, lambda = best_lambda_ridge)
pred_ridge <- predict(ridge_model, as.matrix(validation_set[,-target]))
# Compute RMSE for Ridge
ridge_rmse <- sqrt(mean((validation_set[[target]] - pred_ridge)^2))
cat("Ridge RMSE:", ridge_rmse, "\n")
#plot
plot(ridge_cv)
title("Cross-Validation for Ridge Regression", line = 2.5)
plot(ridge_model, xvar = "lambda", label = TRUE)
title("Coefficient Path for Ridge Regression", line = 2.5)
plot(validation_set[[target]], pred_ridge, main = "Predicted vs Actual (Ridge Regression)",
xlab = "Actual", ylab = "Predicted", col = "blue", pch = 16)
abline(a = 0, b = 1, col = "red")  # Add a 45-degree reference line
ridge_rmse
# Principal Component Regression (PCR)
pcr_model <- pcr(train_set[[target]] ~ ., data = train_set, validation = "CV")
pcr_pred <- predict(pcr_model, validation_set, ncomp = which.min(pcr_model$validation$PRESS))
pcr_rmse <- sqrt(mean((validation_set[[target]] - pcr_pred)^2))
pcr_rmse
# Partial Least Squares (PLS)
pls_model <- plsr(train_set[[target]] ~ ., data = train_set, validation = "CV")
pls_pred <- predict(pls_model, validation_set, ncomp = which.min(pls_model$validation$PRESS))
pls_rmse <- sqrt(mean((validation_set[[target]] - pls_pred)^2))
pls_rmse
# k-Nearest Neighbors
knn_model <- train(train_set[,-target], train_set[[target]], method = "knn", tuneLength = 10)
knn_pred <- predict(knn_model, validation_set[,-target])
knn_rmse <- sqrt(mean((validation_set[[target]] - knn_pred)^2))
# k-Nearest Neighbors
knn_model <- train(train_set[,-target], train_set[[target]], method = "knn", tuneLength = 10)
knn_pred <- predict(knn_model, validation_set[,-target])
knn_rmse <- sqrt(mean((validation_set[[target]] - knn_pred)^2))
knn_rmse
# Polynomial Regression (Degree 2)
poly_model <- lm(train_set[[target]] ~ poly(train_set[, 1], degree = 2))  # Polynomial regression on first predictor
poly_pred <- predict(poly_model, validation_set)
poly_rmse <- sqrt(mean((validation_set[[target]] - poly_pred)^2))  # RMSE for Polynomial Regression
poly_rmse
#Step Function Regression
# Creating step function based on the first predictor variable
step_model <- lm(train_set[[target]] ~ cut(train_set[, 1], breaks = 5))  # Create 5 intervals for step function
step_pred <- predict(step_model, validation_set)
step_rmse <- sqrt(mean((validation_set[[target]] - step_pred)^2))  # RMSE for Step Function Regression
step_rmse
#9 Cubic Splines Regression
# Fit cubic spline model on the first predictor variable
cubic_spline_model <- lm(train_set[[target]] ~ ns(train_set[, 1], df = 4))  # Using 4 degrees of freedom for the spline
cubic_spline_pred <- predict(cubic_spline_model, validation_set)
cubic_spline_rmse <- sqrt(mean((validation_set[[target]] - cubic_spline_pred)^2))  # RMSE for Cubic Splines
cubic_spline_rmse
# Natural Cubic Splines
natural_spline_model <- lm(train_set[[target]] ~ ns(train_set[, 1], df = 5, Boundary.knots = c(min(train_set[, 1]), max(train_set[, 1]))))
# Predict on validation data
natural_spline_pred <- predict(natural_spline_model, newdata = validation_set)
# Calculate RMSE
natural_spline_rmse <- sqrt(mean((validation_set[[target]] - natural_spline_pred)^2))
natural_spline_rmse
# Combine RMSE values of all models
model_comparison_all <- data.frame(
Model = c("Lasso", "Ridge", "PCR", "PLS", "KNN", "Polynomial Regression",
"Step Function Regression", "Cubic Splines", "Natural Cubic Splines"),
RMSE = c(lasso_rmse, ridge_rmse, pcr_rmse, pls_rmse, knn_rmse, poly_rmse,
step_rmse, cubic_spline_rmse, natural_spline_rmse)
)
# Display the performance comparison table
print(model_comparison_all)
# Optionally, sort by RMSE to identify the best-performing model
model_comparison_all <- model_comparison_all[order(model_comparison_all$RMSE), ]
# Print sorted model comparison
print(model_comparison_all)
# Model Comparison
model_comparison <- data.frame(
Model = c("Lasso", "Ridge", "PCR", "PLS"),
RMSE = c(lasso_rmse, ridge_rmse, pcr_rmse, pls_rmse)
)
print(model_comparison)
library(roxygen2)
roxygenize()
git init
git init
git init
file.remove("NAMESPACE")
library(devtools)
devtools::document()
file.remove("NAMESPACE")
library(devtools)
devtools::document()
library(devtools)
devtools::document()
library(roxygen2)
roxygenize()
library(devtools)
devtools::document()
# Run all tests in the package
if (!requireNamespace("testthat", quietly = TRUE)) {
install.packages("testthat")
}
devtools::test()
devtools::check()
devtools::build()
library(roxygen2)
roxygenize()
library(roxygen2)
roxygenize()
# Installation
You can install the development version of the package from GitHub:
# install.packages("devtools")
devtools::install_github("20abh/abdullah")
# install.packages("devtools")
devtools::install_github("20abh/abdullah")
# install.packages("devtools")
devtools::install_github("20abh/abdullah")
# install.packages("devtools")
devtools::install_github("20abh/abdullah")
devtools::install_github("20abh/abdullah", auth_token = "your_personal_access_token")
library(usethis)
# Create R scripts for the functions
use_r("check_assumptions")
use_r("perform_anova")
use_r("generate_anova_table")
# Create test scripts for the functions
use_test("check_assumptions")
use_test("perform_anova")
use_test("generate_anova_table")
library(roxygen2)
roxygenize()
library(devtools)
devtools::document()
# Run all tests in the package
if (!requireNamespace("testthat", quietly = TRUE)) {
install.packages("testthat")
}
devtools::test()
devtools::check()
